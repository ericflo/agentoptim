{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from benchmark import (\n",
    "    INVALID_ANSWER,\n",
    "    NO_ANSWER,\n",
    "    VALID_ANSWER,\n",
    "    aggregate_responses,\n",
    "    calibration_curve,\n",
    "    detect_names_from_dict,\n",
    "    empirical_distr,\n",
    "    extract_predictions,\n",
    "    kl_div,\n",
    "    load_predictions,\n",
    "    load_responses_all,\n",
    "    plot_calibration_curve,\n",
    "    plot_heatmap,\n",
    ")\n",
    "from utils_ext.plot import Plotter\n",
    "from utils_ext.tools import setup_logging\n",
    "\n",
    "plt.ioff()\n",
    "setup_logging()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "PATH_OUTPUT = \"../results\"\n",
    "\n",
    "# setup plotter\n",
    "\n",
    "FONTSIZE_DEFAULT = 6\n",
    "FONTSIZE_SMALL = 5\n",
    "\n",
    "Plotter.setup()\n",
    "Plotter.configure(\n",
    "    basewidth=5.5,\n",
    "    fontsize=FONTSIZE_DEFAULT,\n",
    "    latex=False,\n",
    "    rcparams={\n",
    "        \"lines.linewidth\": 1, # default: 1.5\n",
    "        \"axes.labelpad\": 1, # default: 4\n",
    "    },\n",
    "    save_dir=f\"{PATH_OUTPUT}/plots_paper\",\n",
    "    save_format=\"pdf\",\n",
    ")\n",
    "Plotter.configure(\n",
    "    latex=True,\n",
    "    latex_preamble=\"\\n\".join([\n",
    "        r\"\\usepackage[utf8]{inputenc}\",\n",
    "        r\"\\usepackage[T1]{fontenc}\",\n",
    "        r\"\\usepackage{microtype}\",\n",
    "        r\"\\usepackage{amsmath,amssymb,amsfonts,mathrsfs}\",\n",
    "        r\"\\renewcommand{\\rmdefault}{ptm}\",\n",
    "        r\"\\renewcommand{\\sfdefault}{phv}\",\n",
    "    ]),\n",
    ")\n",
    "\n",
    "Plotter.display_css(\".cell-output-ipywidget-background { background: lightgray !important; }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CACHE = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: load responses and extract predictions\n",
    "responses_all = load_responses_all(f\"{PATH_OUTPUT}/responses\", dataset_cache=DATASET_CACHE)\n",
    "y_true_all, y_pred_all = extract_predictions(responses_all, sample=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2: load predictions only\n",
    "y_true_all, y_pred_all = load_predictions(f\"{PATH_OUTPUT}/predictions_sampled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_ACCURACY = \"tab:blue\"\n",
    "COLOR_CONFIDENCE = \"tab:green\"\n",
    "COLOR_ECE = \"tab:red\"\n",
    "COLOR_CONF_N_DISTINCT = \"darkorange\"\n",
    "COLOR_CONF_VARIANCE = \"orange\"\n",
    "COLOR_KL_DIV = \"mediumpurple\"\n",
    "\n",
    "YLIM_CALIBRATION = (0, 1.12)\n",
    "YLIM_OTHERS_1 = (0, 112)\n",
    "YLIM_OTHERS_2 = (0, 0.23)\n",
    "\n",
    "# plotting utils\n",
    "\n",
    "MODELS_TINY = [\n",
    "    # \"gemma1.1-2b-it\",\n",
    "    \"gemma1.1-7b-it\",\n",
    "    \"llama3-8b-instruct\",\n",
    "    \"qwen1.5-7b-chat\",\n",
    "]\n",
    "MODELS_LARGE = [\n",
    "    \"llama3-70b-instruct\",\n",
    "    \"qwen1.5-32b-chat\",\n",
    "    \"qwen1.5-72b-chat\",\n",
    "    \"qwen1.5-110b-chat\",\n",
    "    \"gpt3.5-turbo\",\n",
    "    \"gpt4o-mini\",\n",
    "    \"gpt4o\",\n",
    "]\n",
    "\n",
    "METHODS_SCORE_RANGE = [\n",
    "    \"basic_1s\",\n",
    "    None,\n",
    "    \"basic_1s_scorefloat\",\n",
    "    None,\n",
    "    \"basic_1s_scoreletter\",\n",
    "    None,\n",
    "    \"basic_1s_scoretext\",\n",
    "]\n",
    "METHODS_SCORE_FORMULATION = [\n",
    "    \"basic_1s\",\n",
    "    \"basic_1s_probscore\",\n",
    "    None,\n",
    "    \"advanced_1s\",\n",
    "    \"advanced_1s_probscore\",\n",
    "    None,\n",
    "    \"tian2023just_1s_top1_v3\",\n",
    "    \"tian2023just_1s_top1_v2\",\n",
    "    \"tian2023just_1s_top1\",\n",
    "]\n",
    "METHODS_ADVANCED_FORMULATION = [\n",
    "    \"basic_1s\",\n",
    "    \"advanced_1s\",\n",
    "    None,\n",
    "    \"basic_1s_probscore\",\n",
    "    \"advanced_1s_probscore\",\n",
    "]\n",
    "METHODS_FEW_SHOT = [\n",
    "    \"basic_1s\",\n",
    "    \"basic_1s_1shot\",\n",
    "    \"basic_1s_5shot\",\n",
    "]\n",
    "METHODS_OTHERS = [\n",
    "    \"tian2023just_1s_top1\",\n",
    "    \"tian2023just_1s_top1_v1\",\n",
    "    None,\n",
    "    \"tian2023just_1s_top1\",\n",
    "    \"tian2023just_1s_top4\",\n",
    "    None,\n",
    "    \"xiong2023can_vanilla\",\n",
    "    \"xiong2023can_cot\",\n",
    "]\n",
    "METHODS_COMBO = [\n",
    "    \"basic_1s\",\n",
    "    None,\n",
    "    \"basic_1s_probscore\",\n",
    "    None,\n",
    "    \"advanced_1s\",\n",
    "    None,\n",
    "    \"basic_1s_5shot\",\n",
    "    None,\n",
    "    \"combo_1s_v2\",\n",
    "]\n",
    "\n",
    "DATASET_NAME_ALIASES = {\n",
    "    \"arc-c\":           \"arc-c\",\n",
    "    \"arc-e\":           \"arc-e\",\n",
    "    \"commonsense_qa\":  \"commonsense_qa\",\n",
    "    \"imdb\":            \"imdb\",\n",
    "    \"logi_qa\":         \"logi_qa\",\n",
    "    \"mmlu\":            \"mmlu\",\n",
    "    \"sciq\":            \"sciq\",\n",
    "    \"social_i_qa\":     \"social_i_qa\",\n",
    "    \"trivia_qa\":       \"trivia_qa\",\n",
    "    \"truthful_qa-mc1\": \"truthful_qa-mc1\",\n",
    "    \"truthful_qa-mc2\": \"truthful_qa-mc2\",\n",
    "}\n",
    "MODEL_NAME_ALIASES = {\n",
    "    \"gemma1.1-2b-it\":      \"gemma1.1-2b\",\n",
    "    \"gemma1.1-7b-it\":      \"gemma1.1-7b\",\n",
    "    \"llama3-8b-instruct\":  \"llama3-8b\",\n",
    "    \"llama3-70b-instruct\": \"llama3-70b\",\n",
    "    \"qwen1.5-7b-chat\":     \"qwen1.5-7b\",\n",
    "    \"qwen1.5-32b-chat\":    \"qwen1.5-32b\",\n",
    "    \"qwen1.5-72b-chat\":    \"qwen1.5-72b\",\n",
    "    \"qwen1.5-110b-chat\":   \"qwen1.5-110b\",\n",
    "    \"gpt3.5-turbo\":        \"gpt3.5-turbo\",\n",
    "    \"gpt4o-mini\":          \"gpt4o-mini\",\n",
    "    \"gpt4o\":               \"gpt4o\",\n",
    "}\n",
    "METHOD_NAME_ALIASES = {\n",
    "    \"basic_1s\":                  \"basic\",\n",
    "    \"basic_1s_scorefloat\":       \"basic_scorefloat\",\n",
    "    \"basic_1s_scoreletter\":      \"basic_scoreletter\",\n",
    "    \"basic_1s_scoretext\":        \"basic_scoretext\",\n",
    "    \"basic_1s_probscore\":        \"basic_probscore\",\n",
    "    \"basic_1s_1shot\":            \"basic_1shot\",\n",
    "    \"basic_1s_5shot\":            \"basic_5shot\",\n",
    "    \"advanced_1s\":               \"advanced\",\n",
    "    \"advanced_1s_probscore\":     \"advanced_probscore\",\n",
    "    \"combo_1s_v2\":               \"combo\",\n",
    "    \"tian2023just_1s_top1\":      \"tian2023_top1\",\n",
    "    \"tian2023just_1s_top1_v1\":   \"tian2023_top1_v1\",\n",
    "    \"tian2023just_1s_top1_v2\":   \"tian2023_top1_v2\",\n",
    "    \"tian2023just_1s_top1_v3\":   \"tian2023_top1_v3\",\n",
    "    \"tian2023just_1s_top4\":      \"tian2023_top4\",\n",
    "    \"xiong2023can_vanilla\":      \"xiong2023_vanilla\",\n",
    "    \"xiong2023can_cot\":          \"xiong2023_cot\",\n",
    "}\n",
    "\n",
    "def translate(names, aliases):\n",
    "    if isinstance(names, str):\n",
    "        return aliases[names]\n",
    "    else:\n",
    "        return [aliases[name] for name in names]\n",
    "def translate_dataset(names):\n",
    "    return translate(names, DATASET_NAME_ALIASES)\n",
    "def translate_model(names):\n",
    "    return translate(names, MODEL_NAME_ALIASES)\n",
    "def translate_method(names):\n",
    "    return translate(names, METHOD_NAME_ALIASES)\n",
    "\n",
    "def extract_none_indices(names):\n",
    "    none_indices = [i for i, name in enumerate(names) if name is None]\n",
    "    none_indices = list(none_indices - np.arange(len(none_indices)))\n",
    "    names = [name for name in names if name is not None]\n",
    "    return names, none_indices\n",
    "\n",
    "def plot_grouped_bar(axes_scores_args, width=None, alpha=None, with_labels=False, with_lines=False, none_indices=[]):\n",
    "    def filter_singles(l):\n",
    "        l_new = []\n",
    "        for i in range(len(l)):\n",
    "            if not np.isnan(l[i]):\n",
    "                left_is_val = i-1 >= 0 and not np.isnan(l[i-1])\n",
    "                right_is_val = i+1 < len(l) and not np.isnan(l[i+1])\n",
    "                if left_is_val:\n",
    "                    l_new.append(l[i])\n",
    "                elif right_is_val:\n",
    "                    if len(l_new) > 0:\n",
    "                        l_new.append(np.nan)\n",
    "                    l_new.append(l[i])\n",
    "        return np.asarray(l_new)\n",
    "\n",
    "    n_bars = len(axes_scores_args)\n",
    "    n_scores = len(axes_scores_args[0][1])\n",
    "    if width is None:\n",
    "        width = 1 / (n_bars + 1.5)\n",
    "    if alpha is None:\n",
    "        alpha = 0.5 if with_lines else 1\n",
    "\n",
    "    x = np.arange(n_scores, dtype=float)\n",
    "    offset = (n_bars - 1) / 2\n",
    "    # plot bars\n",
    "    for i, (ax, scores, args) in enumerate(axes_scores_args):\n",
    "        bar_container = ax.bar(x+(i-offset)*width, scores, width, color=args[\"color\"], alpha=alpha, label=args[\"label\"])\n",
    "        if with_labels:\n",
    "            ax.bar_label(bar_container, fmt=args[\"fmt\"], fontsize=FONTSIZE_SMALL, padding=3, rotation=90)\n",
    "    # plot lines\n",
    "    if with_lines:\n",
    "        x_ = filter_singles(np.insert(x, none_indices, np.nan))\n",
    "        for i, (ax, scores, args) in enumerate(axes_scores_args):\n",
    "            ax.plot(x_+(i-offset)*width, filter_singles(np.insert(scores, none_indices, np.nan)), color=args[\"color\"], marker=\"o\", markersize=2)\n",
    "\n",
    "def annotate_agg_over(ax, agg_over):\n",
    "    ax.text(1.0, 1.02, f\"agg. over {agg_over}\", ha=\"right\", va=\"bottom\", fontsize=FONTSIZE_SMALL, transform=ax.transAxes)\n",
    "\n",
    "# plotting functions\n",
    "\n",
    "def make_plots_datasets(y_true_all, y_pred_all, agg_over, dataset_names=None, model_names=None, method_names=None, sort_by=None, label_rotation=45, n_bins=20):\n",
    "    dataset_names, model_names, method_names = detect_names_from_dict(y_true_all, dataset_names=dataset_names, model_names=model_names, method_names=method_names)\n",
    "\n",
    "    # compute\n",
    "    scores = {\n",
    "        \"ece\": np.zeros(len(dataset_names)),\n",
    "        \"accuracy\": np.zeros(len(dataset_names)),\n",
    "        \"confidence\": np.zeros(len(dataset_names)),\n",
    "        \"confidence_n_distinct\": np.zeros(len(dataset_names)),\n",
    "        \"confidence_variance\": np.zeros(len(dataset_names)),\n",
    "        \"kl_div_over_dataset\": np.zeros(len(dataset_names)),\n",
    "    }\n",
    "    for i, dataset_name in enumerate(dataset_names):\n",
    "        y_true = aggregate_responses(y_true_all, dataset_name, model_names, method_names)\n",
    "        y_pred = aggregate_responses(y_pred_all, dataset_name, model_names, method_names)\n",
    "\n",
    "        prob_true, prob_pred, bins, bin_count = calibration_curve(y_true, y_pred, n_bins=n_bins)\n",
    "        ece = np.sum(bin_count / len(y_true) * np.abs(prob_true - prob_pred), where=bin_count > 0)\n",
    "        scores[\"ece\"][i] = ece\n",
    "        scores[\"accuracy\"][i] = np.mean(y_true)\n",
    "        scores[\"confidence\"][i] = np.mean(y_pred)\n",
    "        scores[\"confidence_n_distinct\"][i] = len(np.unique(y_pred))\n",
    "        scores[\"confidence_variance\"][i] = np.std(y_pred)\n",
    "        scores[\"kl_div_over_dataset\"][i] = np.mean([\n",
    "            kl_div(\n",
    "                empirical_distr(aggregate_responses(y_pred_all, dataset_name, model_name, method_name), n_bins),\n",
    "                empirical_distr(aggregate_responses(y_pred_all, dataset_names, model_name, method_name), n_bins),\n",
    "            )\n",
    "            for model_name in model_names\n",
    "            for method_name in method_names\n",
    "        ])\n",
    "\n",
    "    # sort\n",
    "    if sort_by is not None:\n",
    "        scores_zipped = zip(scores[sort_by], dataset_names, scores[\"accuracy\"], scores[\"confidence\"], scores[\"ece\"], scores[\"confidence_n_distinct\"], scores[\"confidence_variance\"], scores[\"kl_div_over_dataset\"])\n",
    "        scores_zipped = sorted(scores_zipped, reverse=True)\n",
    "        _, dataset_names, scores[\"accuracy\"], scores[\"confidence\"], scores[\"ece\"], scores[\"confidence_n_distinct\"], scores[\"confidence_variance\"], scores[\"kl_div_over_dataset\"] = zip(*scores_zipped)\n",
    "\n",
    "    # plot\n",
    "    x = np.arange(len(dataset_names))\n",
    "\n",
    "    fig1, ax = Plotter.create()\n",
    "    Plotter.set(\n",
    "        ax,\n",
    "        ylim=YLIM_CALIBRATION,\n",
    "        xticks=dict(ticks=x, labels=translate_dataset(dataset_names), rotation=label_rotation, ha=\"right\"),\n",
    "    )\n",
    "    plot_grouped_bar([\n",
    "        (ax, scores[\"accuracy\"], dict(label=\"accuracy\", color=COLOR_ACCURACY)),\n",
    "        (ax, scores[\"confidence\"], dict(label=\"confidence\", color=COLOR_CONFIDENCE)),\n",
    "        (ax, scores[\"ece\"], dict(label=\"ECE\", color=COLOR_ECE)),\n",
    "    ], with_lines=True)\n",
    "    annotate_agg_over(ax, agg_over)\n",
    "\n",
    "    fig2, ax1 = Plotter.create()\n",
    "    ax2 = ax1.twinx()\n",
    "    Plotter.set(\n",
    "        ax1,\n",
    "        ylim=YLIM_OTHERS_1,\n",
    "        xticks=dict(ticks=x, labels=translate_dataset(dataset_names), rotation=label_rotation, ha=\"right\"),\n",
    "    )\n",
    "    Plotter.set(\n",
    "        ax2,\n",
    "        ylim=YLIM_OTHERS_2,\n",
    "    )\n",
    "    plot_grouped_bar([\n",
    "        (ax1, scores[\"confidence_n_distinct\"], dict(label=\"# distinct\", color=COLOR_CONF_N_DISTINCT)),\n",
    "        (ax2, scores[\"confidence_variance\"], dict(label=\"variance\", color=COLOR_CONF_VARIANCE)),\n",
    "        (ax2, scores[\"kl_div_over_dataset\"], dict(label=\"kl_div\", color=COLOR_KL_DIV)),\n",
    "    ], with_lines=True)\n",
    "    annotate_agg_over(ax1, agg_over)\n",
    "\n",
    "    return fig1, fig2\n",
    "\n",
    "def make_plots_models(y_true_all, y_pred_all, agg_over, model_names=None, method_names=None, sort_by=None, label_rotation=45, n_bins=20):\n",
    "    if model_names is not None:\n",
    "        model_names, none_indices = extract_none_indices(model_names)\n",
    "    else:\n",
    "        none_indices = []\n",
    "\n",
    "    dataset_names, model_names, method_names = detect_names_from_dict(y_true_all, model_names=model_names, method_names=method_names)\n",
    "\n",
    "    # compute\n",
    "    scores = {\n",
    "        \"ece\": np.zeros(len(model_names)),\n",
    "        \"accuracy\": np.zeros(len(model_names)),\n",
    "        \"confidence\": np.zeros(len(model_names)),\n",
    "        \"confidence_n_distinct\": np.zeros(len(model_names)),\n",
    "        \"confidence_variance\": np.zeros(len(model_names)),\n",
    "        \"kl_div_over_dataset\": np.zeros(len(model_names)),\n",
    "    }\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        y_true = aggregate_responses(y_true_all, dataset_names, model_name, method_names)\n",
    "        y_pred = aggregate_responses(y_pred_all, dataset_names, model_name, method_names)\n",
    "\n",
    "        prob_true, prob_pred, bins, bin_count = calibration_curve(y_true, y_pred, n_bins=n_bins)\n",
    "        ece = np.sum(bin_count / len(y_true) * np.abs(prob_true - prob_pred), where=bin_count > 0)\n",
    "        scores[\"ece\"][i] = ece\n",
    "        scores[\"accuracy\"][i] = np.mean(y_true)\n",
    "        scores[\"confidence\"][i] = np.mean(y_pred)\n",
    "        scores[\"confidence_n_distinct\"][i] = len(np.unique(y_pred))\n",
    "        scores[\"confidence_variance\"][i] = np.std(y_pred)\n",
    "        scores[\"kl_div_over_dataset\"][i] = np.mean([\n",
    "            kl_div(\n",
    "                empirical_distr(aggregate_responses(y_pred_all, dataset_name, model_name, method_name), n_bins),\n",
    "                empirical_distr(aggregate_responses(y_pred_all, dataset_names, model_name, method_name), n_bins),\n",
    "            )\n",
    "            for dataset_name in dataset_names\n",
    "            for method_name in method_names\n",
    "        ])\n",
    "\n",
    "    # sort\n",
    "    if sort_by is not None:\n",
    "        scores_zipped = zip(scores[sort_by], model_names, scores[\"accuracy\"], scores[\"confidence\"], scores[\"ece\"], scores[\"confidence_n_distinct\"], scores[\"confidence_variance\"], scores[\"kl_div_over_dataset\"])\n",
    "        scores_zipped = sorted(scores_zipped, reverse=True)\n",
    "        _, model_names, scores[\"accuracy\"], scores[\"confidence\"], scores[\"ece\"], scores[\"confidence_n_distinct\"], scores[\"confidence_variance\"], scores[\"kl_div_over_dataset\"] = zip(*scores_zipped)\n",
    "\n",
    "    # plot\n",
    "    x = np.arange(len(model_names), dtype=float)\n",
    "\n",
    "    fig1, ax = Plotter.create()\n",
    "    Plotter.set(\n",
    "        ax,\n",
    "        ylim=YLIM_CALIBRATION,\n",
    "        xticks=dict(ticks=x, labels=translate_model(model_names), rotation=label_rotation, ha=\"right\"),\n",
    "    )\n",
    "    plot_grouped_bar([\n",
    "        (ax, scores[\"accuracy\"], dict(label=\"accuracy\", color=COLOR_ACCURACY, fmt=\"{:.2f}\")),\n",
    "        (ax, scores[\"confidence\"], dict(label=\"confidence\", color=COLOR_CONFIDENCE, fmt=\"{:.2f}\")),\n",
    "        (ax, scores[\"ece\"], dict(label=\"ECE\", color=COLOR_ECE, fmt=\"{:.2f}\")),\n",
    "    ], with_lines=True, none_indices=none_indices)\n",
    "    annotate_agg_over(ax, agg_over)\n",
    "\n",
    "    fig2, ax1 = Plotter.create()\n",
    "    ax2 = ax1.twinx()\n",
    "    Plotter.set(\n",
    "        ax1,\n",
    "        ylim=YLIM_OTHERS_1,\n",
    "        xticks=dict(ticks=x, labels=translate_model(model_names), rotation=label_rotation, ha=\"right\"),\n",
    "    )\n",
    "    Plotter.set(\n",
    "        ax2,\n",
    "        ylim=YLIM_OTHERS_2,\n",
    "    )\n",
    "    plot_grouped_bar([\n",
    "        (ax1, scores[\"confidence_n_distinct\"], dict(label=\"# distinct\", color=COLOR_CONF_N_DISTINCT)),\n",
    "        (ax2, scores[\"confidence_variance\"], dict(label=\"variance\", color=COLOR_CONF_VARIANCE)),\n",
    "        (ax2, scores[\"kl_div_over_dataset\"], dict(label=\"kl_div\", color=COLOR_KL_DIV)),\n",
    "    ], with_lines=True, none_indices=none_indices)\n",
    "    annotate_agg_over(ax1, agg_over)\n",
    "\n",
    "    return fig1, fig2\n",
    "\n",
    "def make_plots_methods(y_true_all, y_pred_all, agg_over, model_names=None, method_names=None, label_rotation=45, single_plot=False, n_bins=20):\n",
    "    if method_names is not None:\n",
    "        method_names, none_indices = extract_none_indices(method_names)\n",
    "    else:\n",
    "        none_indices = []\n",
    "\n",
    "    dataset_names, model_names, method_names = detect_names_from_dict(\n",
    "        y_true_all,\n",
    "        model_names=model_names,\n",
    "        method_names=method_names,\n",
    "    )\n",
    "\n",
    "    # compute\n",
    "    scores = {\n",
    "        \"ece\": np.zeros(len(method_names)),\n",
    "        \"accuracy\": np.zeros(len(method_names)),\n",
    "        \"confidence\": np.zeros(len(method_names)),\n",
    "        \"confidence_n_distinct\": np.zeros(len(method_names)),\n",
    "        \"confidence_variance\": np.zeros(len(method_names)),\n",
    "        \"kl_div_over_dataset\": np.zeros((len(method_names))),\n",
    "    }\n",
    "    for i, method_name in enumerate(method_names):\n",
    "        y_true = aggregate_responses(y_true_all, dataset_names, model_names, method_name)\n",
    "        y_pred = aggregate_responses(y_pred_all, dataset_names, model_names, method_name)\n",
    "\n",
    "        prob_true, prob_pred, bins, bin_count = calibration_curve(y_true, y_pred, n_bins=n_bins)\n",
    "        ece = np.sum(bin_count / len(y_true) * np.abs(prob_true - prob_pred), where=bin_count > 0)\n",
    "        scores[\"ece\"][i] = ece\n",
    "        scores[\"accuracy\"][i] = np.mean(y_true)\n",
    "        scores[\"confidence\"][i] = np.mean(y_pred)\n",
    "        scores[\"confidence_n_distinct\"][i] = len(np.unique(y_pred))\n",
    "        scores[\"confidence_variance\"][i] = np.std(y_pred)\n",
    "        scores[\"kl_div_over_dataset\"][i] = np.mean([\n",
    "            kl_div(\n",
    "                empirical_distr(aggregate_responses(y_pred_all, dataset_name, model_name, method_name), n_bins),\n",
    "                empirical_distr(aggregate_responses(y_pred_all, dataset_names, model_name, method_name), n_bins),\n",
    "            )\n",
    "            for dataset_name in dataset_names\n",
    "            for model_name in model_names\n",
    "        ])\n",
    "\n",
    "    # plot\n",
    "    x = np.arange(len(method_names), dtype=float)\n",
    "\n",
    "    fig1, ax = Plotter.create()\n",
    "    fig2, ax1 = Plotter.create()\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    if single_plot:\n",
    "        Plotter.set(ax, xticks=[])\n",
    "    else:\n",
    "        Plotter.set(ax, xticks=dict(ticks=x, labels=translate_method(method_names), rotation=label_rotation, ha=\"right\"))\n",
    "    Plotter.set(\n",
    "        ax,\n",
    "        ylim=YLIM_CALIBRATION,\n",
    "    )\n",
    "    plot_grouped_bar([\n",
    "        (ax, scores[\"accuracy\"], dict(label=\"accuracy\", color=COLOR_ACCURACY, fmt=\"{:.2f}\")),\n",
    "        (ax, scores[\"confidence\"], dict(label=\"confidence\", color=COLOR_CONFIDENCE, fmt=\"{:.2f}\")),\n",
    "        (ax, scores[\"ece\"], dict(label=\"ECE\", color=COLOR_ECE, fmt=\"{:.2f}\")),\n",
    "    ], with_labels=True, with_lines=True, none_indices=none_indices)\n",
    "    annotate_agg_over(ax, agg_over)\n",
    "\n",
    "    Plotter.set(\n",
    "        ax1,\n",
    "        ylim=YLIM_OTHERS_1,\n",
    "        xticks=dict(ticks=x, labels=translate_method(method_names), rotation=label_rotation, ha=\"right\"),\n",
    "    )\n",
    "    Plotter.set(\n",
    "        ax2,\n",
    "        ylim=YLIM_OTHERS_2,\n",
    "    )\n",
    "    plot_grouped_bar([\n",
    "        (ax1, scores[\"confidence_n_distinct\"], dict(label=\"n_distinct\", color=COLOR_CONF_N_DISTINCT, fmt=\"{:.0f}\")),\n",
    "        (ax2, scores[\"confidence_variance\"], dict(label=\"variance\", color=COLOR_CONF_VARIANCE, fmt=\"{:.2f}\")),\n",
    "        (ax2, scores[\"kl_div_over_dataset\"], dict(label=\"kl_div\", color=COLOR_KL_DIV, fmt=\"{:.2f}\")),\n",
    "    ], with_labels=True, with_lines=True, none_indices=none_indices)\n",
    "    if not single_plot:\n",
    "        annotate_agg_over(ax1, agg_over)\n",
    "\n",
    "    return fig1, fig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotter.configure(save_always=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(y_true_all, y_pred_all):\n",
    "    dataset_names_all = [\n",
    "        \"sciq\",\n",
    "        \"arc-e\",\n",
    "        \"arc-c\",\n",
    "        \"commonsense_qa\",\n",
    "        \"social_i_qa\",\n",
    "        \"mmlu\",\n",
    "        \"trivia_qa\",\n",
    "        \"truthful_qa-mc1\",\n",
    "        \"truthful_qa-mc2\",\n",
    "        \"logi_qa\",\n",
    "    ]\n",
    "    model_names_all = [\n",
    "        \"gemma1.1-2b-it\",\n",
    "        \"gemma1.1-7b-it\",\n",
    "        None,\n",
    "        \"llama3-8b-instruct\",\n",
    "        \"llama3-70b-instruct\",\n",
    "        None,\n",
    "        \"qwen1.5-7b-chat\",\n",
    "        \"qwen1.5-32b-chat\",\n",
    "        \"qwen1.5-72b-chat\",\n",
    "        \"qwen1.5-110b-chat\",\n",
    "        None,\n",
    "        \"gpt3.5-turbo\",\n",
    "        \"gpt4o-mini\",\n",
    "        \"gpt4o\",\n",
    "    ]\n",
    "\n",
    "    plots = []\n",
    "\n",
    "    fig1, fig2 = make_plots_datasets(y_true_all, y_pred_all, \"models[all], methods[all]\", dataset_names=dataset_names_all, label_rotation=35)\n",
    "    fig1.axes[0].legend(loc=\"center left\")\n",
    "    plots.append((fig1, \"datasets-calibration\"))\n",
    "    # plots.append((fig2, \"datasets-others\"))\n",
    "\n",
    "    fig1, fig2 = make_plots_models(y_true_all, y_pred_all, \"datasets[all], methods[all]\", model_names=model_names_all)\n",
    "    plots.append((fig1, \"models-calibration\"))\n",
    "    # plots.append((fig2, \"models-others\"))\n",
    "\n",
    "    Plotter.finish(\n",
    "        plots, figwidth=0.5,\n",
    "        grid_ncols=2, consistent_size=True,\n",
    "        # save=True,\n",
    "    )\n",
    "\n",
    "plot(y_true_all, y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(y_true_all, y_pred_all):\n",
    "    plots = []\n",
    "\n",
    "    fig1, fig2 = make_plots_methods(y_true_all, y_pred_all, \"datasets[all], models[tiny]\", model_names=MODELS_TINY, method_names=METHODS_COMBO, label_rotation=25, single_plot=True)\n",
    "    h1, l1 = fig2.axes[0].get_legend_handles_labels()\n",
    "    h2, l2 = fig2.axes[1].get_legend_handles_labels()\n",
    "    fig2.axes[0].legend(h1+h2[:1], l1+l2[:1], loc=\"upper left\")\n",
    "    fig2.axes[1].legend(h2[1:], l2[1:], loc=\"upper right\")\n",
    "    plots.append((fig1, f\"methods-calibration-combo-tinymodels\"))\n",
    "    plots.append((fig2, f\"methods-others-combo-tinymodels\"))\n",
    "\n",
    "    fig1, fig2 = make_plots_methods(y_true_all, y_pred_all, \"datasets[all], models[large]\", model_names=MODELS_LARGE, method_names=METHODS_COMBO, label_rotation=25, single_plot=True)\n",
    "    fig1.axes[0].legend(loc=\"center right\")\n",
    "    plots.append((fig1, f\"methods-calibration-combo-largemodels\"))\n",
    "    plots.append((fig2, f\"methods-others-combo-largemodels\"))\n",
    "    plots[::2], plots[1::2] = plots[:2], plots[2:]\n",
    "\n",
    "    Plotter.finish(\n",
    "        plots, figwidth=0.5,\n",
    "        grid_ncols=2, consistent_size=True,\n",
    "        # save=True,\n",
    "    )\n",
    "\n",
    "plot(y_true_all, y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(y_true_all, y_pred_all):\n",
    "    def make_plot(y_true_all, y_pred_all, agg_over, model_name, method_names):\n",
    "        dataset_names, _, _ = detect_names_from_dict(y_true_all)\n",
    "\n",
    "        fig, axes = Plotter.create(ncols=len(method_names), sharey=True)\n",
    "        fig.supxlabel(\"confidence\")\n",
    "        axes[0].set_ylabel(\"accuracy\")\n",
    "\n",
    "        for ax, method_name in zip(axes, method_names):\n",
    "            y_true = aggregate_responses(y_true_all, dataset_names, model_name, method_name)\n",
    "            y_pred = aggregate_responses(y_pred_all, dataset_names, model_name, method_name)\n",
    "            plot_calibration_curve(ax, y_true, y_pred, n_bins=20, labelfmt=\"{:.2f}\", labelsize=FONTSIZE_DEFAULT)\n",
    "\n",
    "        annotate_agg_over(axes[-1], agg_over)\n",
    "\n",
    "        return fig\n",
    "\n",
    "    plots = []\n",
    "\n",
    "    fig = make_plot(y_true_all, y_pred_all, \"datasets[all]\", \"llama3-8b-instruct\", [\"basic_1s\", \"combo_1s_v2\"])\n",
    "    plots.append((fig, \"methods-calibration_curve-basic_vs_combo-llama3_8b\"))\n",
    "    fig = make_plot(y_true_all, y_pred_all, \"datasets[all]\", \"gpt4o\", [\"basic_1s\", \"combo_1s_v2\"])\n",
    "    plots.append((fig, \"methods-calibration_curve-basic_vs_combo-gpt4o\"))\n",
    "\n",
    "    Plotter.finish(\n",
    "        plots, figwidth=0.5, axratio=1,\n",
    "        consistent_size=True,\n",
    "        # save=True,\n",
    "    )\n",
    "\n",
    "plot(y_true_all, y_pred_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(responses_all):\n",
    "    dataset_names, model_names, method_names = detect_names_from_dict(responses_all)\n",
    "\n",
    "    # compute\n",
    "    answer_stats = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: (0, 0, 0))))\n",
    "    answer_stats_over_dataset = defaultdict(lambda: defaultdict(lambda: (0, 0, 0)))\n",
    "    for model_name in model_names:\n",
    "        for method_name in method_names:\n",
    "            for dataset_name in dataset_names:\n",
    "                responses = responses_all[dataset_name][model_name][method_name]\n",
    "                if responses is None:\n",
    "                    continue\n",
    "                answer_stats[model_name][method_name][dataset_name] = (\n",
    "                    len(responses[VALID_ANSWER]),\n",
    "                    len(responses[NO_ANSWER]),\n",
    "                    len(responses[INVALID_ANSWER]),\n",
    "                )\n",
    "            answer_stats_over_dataset[model_name][method_name] = (\n",
    "                sum(answer_stats[model_name][method_name][dataset_name][0] for dataset_name in dataset_names),\n",
    "                sum(answer_stats[model_name][method_name][dataset_name][1] for dataset_name in dataset_names),\n",
    "                sum(answer_stats[model_name][method_name][dataset_name][2] for dataset_name in dataset_names),\n",
    "            )\n",
    "\n",
    "    percentage_valid_answer = np.zeros((len(method_names), len(model_names)))\n",
    "    percentage_no_answer = np.zeros((len(method_names), len(model_names)))\n",
    "    for i, method_name in enumerate(method_names):\n",
    "        for j, model_name in enumerate(model_names):\n",
    "            n_valid_answer, n_no_answer, n_invalid_answer = answer_stats_over_dataset[model_name][method_name]\n",
    "            n_total = n_valid_answer + n_no_answer + n_invalid_answer\n",
    "            percentage_valid_answer[i, j] = n_valid_answer / n_total if n_total > 0 else None\n",
    "            percentage_no_answer[i, j] = n_no_answer / n_total if n_total > 0 else None\n",
    "\n",
    "    plots = []\n",
    "\n",
    "    fig, ax = Plotter.create()\n",
    "    plot_heatmap(ax, percentage_valid_answer, translate_method(method_names), translate_model(model_names), plot_mean=False, format=\"{:.2f}\", vmin=0, vmax=1.3, cmap=\"Greens\")\n",
    "    plots.append((fig, \"answer_statistics\"))\n",
    "\n",
    "    # fig, ax = Plotter.create()\n",
    "    # plot_heatmap(ax, percentage_no_answer, method_names, model_names, plot_mean=False, format=\"{:.2f}\", vmin=0, vmax=0.3, cmap=\"Reds\")\n",
    "    # plots.append((fig, \"answer_statistics-no_answers\"))\n",
    "\n",
    "    Plotter.finish(\n",
    "        plots, figwidth=0.59, axratio=len(method_names) / len(model_names),\n",
    "        save=True,\n",
    "    )\n",
    "\n",
    "plot(responses_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(y_true_all, y_pred_all):\n",
    "    dataset_names_all = [\n",
    "        \"sciq\",\n",
    "        \"arc-e\",\n",
    "        \"arc-c\",\n",
    "        \"commonsense_qa\",\n",
    "        \"social_i_qa\",\n",
    "        \"mmlu\",\n",
    "        \"trivia_qa\",\n",
    "        \"truthful_qa-mc1\",\n",
    "        \"truthful_qa-mc2\",\n",
    "        \"logi_qa\",\n",
    "    ]\n",
    "\n",
    "    plots = []\n",
    "\n",
    "    fig1, fig2 = make_plots_datasets(y_true_all, y_pred_all, \"models[tiny], methods[all]\", dataset_names=dataset_names_all, model_names=MODELS_TINY, label_rotation=35)\n",
    "    fig1.axes[0].legend(loc=\"center left\")\n",
    "    plots.append((fig1, \"datasets-calibration-tinymodels\"))\n",
    "    # plots.append((fig2, \"datasets-others-tinymodels\"))\n",
    "\n",
    "    fig1, fig2 = make_plots_datasets(y_true_all, y_pred_all, \"models[large], methods[all]\", dataset_names=dataset_names_all, model_names=MODELS_LARGE, label_rotation=35)\n",
    "    plots.append((fig1, \"datasets-calibration-largemodels\"))\n",
    "    # plots.append((fig2, \"datasets-others-largemodels\"))\n",
    "\n",
    "    Plotter.finish(\n",
    "        plots, figwidth=0.5,\n",
    "        grid_ncols=2, consistent_size=True,\n",
    "        # save=True,\n",
    "    )\n",
    "\n",
    "plot(y_true_all, y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def plot(y_true_all, y_pred_all):\n",
    "    dataset_names, _, method_names = detect_names_from_dict(y_true_all)\n",
    "\n",
    "    fig, ax = Plotter.create()\n",
    "    ax.set_xlabel(\"confidence\")\n",
    "    ax.set_ylabel(\"accuracy\")\n",
    "\n",
    "    y_true = aggregate_responses(y_true_all, dataset_names, \"gemma1.1-2b-it\", method_names)\n",
    "    y_pred = aggregate_responses(y_pred_all, dataset_names, \"gemma1.1-2b-it\", method_names)\n",
    "    plot_calibration_curve(ax, y_true, y_pred, n_bins=20, labelfmt=\"{:.2f}\", labelsize=FONTSIZE_DEFAULT)\n",
    "    annotate_agg_over(ax, \"datasets[all], methods[all]\")\n",
    "\n",
    "    Plotter.finish(\n",
    "        (fig, \"models-calibration_curve-gemma1_1_2b\"), figwidth=0.4, axratio=1,\n",
    "        # save=True,\n",
    "    )\n",
    "\n",
    "plot(y_true_all, y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(y_true_all, y_pred_all):\n",
    "    def make_plots_all(model_names, agg_over, suffix=\"\"):\n",
    "        plots = []\n",
    "\n",
    "        fig1, fig2 = make_plots_methods(y_true_all, y_pred_all, agg_over, model_names=model_names, method_names=METHODS_SCORE_RANGE, label_rotation=30, single_plot=True)\n",
    "        plots.append((fig1, f\"methods-calibration-score_range{suffix}\"))\n",
    "        plots.append((fig2, f\"methods-others-score_range{suffix}\"))\n",
    "        fig1, fig2 = make_plots_methods(y_true_all, y_pred_all, agg_over, model_names=model_names, method_names=METHODS_SCORE_FORMULATION, label_rotation=30, single_plot=True)\n",
    "        plots.append((fig1, f\"methods-calibration-score_formulation{suffix}\"))\n",
    "        plots.append((fig2, f\"methods-others-score_formulation{suffix}\"))\n",
    "        fig1, fig2 = make_plots_methods(y_true_all, y_pred_all, agg_over, model_names=model_names, method_names=METHODS_ADVANCED_FORMULATION, label_rotation=30, single_plot=True)\n",
    "        plots.append((fig1, f\"methods-calibration-advanced_formulation{suffix}\"))\n",
    "        plots.append((fig2, f\"methods-others-advanced_formulation{suffix}\"))\n",
    "        fig1, fig2 = make_plots_methods(y_true_all, y_pred_all, agg_over, model_names=model_names, method_names=METHODS_FEW_SHOT, label_rotation=30, single_plot=True)\n",
    "        plots.append((fig1, f\"methods-calibration-few_shot{suffix}\"))\n",
    "        plots.append((fig2, f\"methods-others-few_shot{suffix}\"))\n",
    "        fig1, fig2 = make_plots_methods(y_true_all, y_pred_all, agg_over, model_names=model_names, method_names=METHODS_OTHERS, label_rotation=30, single_plot=True)\n",
    "        plots.append((fig1, f\"methods-calibration-others{suffix}\"))\n",
    "        plots.append((fig2, f\"methods-others-others{suffix}\"))\n",
    "\n",
    "        return plots\n",
    "\n",
    "    plots_tiny = make_plots_all(MODELS_TINY, \"datasets[all], models[tiny]\", suffix=\"-tinymodels\")\n",
    "    for (fig1, _), (fig2, _) in zip(plots_tiny[::2], plots_tiny[1::2]):\n",
    "        h1, l1 = fig2.axes[0].get_legend_handles_labels()\n",
    "        h2, l2 = fig2.axes[1].get_legend_handles_labels()\n",
    "        fig2.axes[0].legend(h1+h2[:1], l1+l2[:1], loc=\"upper left\")\n",
    "        fig2.axes[1].legend(h2[1:], l2[1:], loc=\"upper right\")\n",
    "\n",
    "    plots_large = make_plots_all(MODELS_LARGE, \"datasets[all], models[large]\", suffix=\"-largemodels\")\n",
    "    for (fig1, _), (fig2, _) in zip(plots_large[::2], plots_large[1::2]):\n",
    "        fig1.axes[0].legend(loc=\"center right\")\n",
    "\n",
    "    plots = plots_tiny + plots_large\n",
    "    plots[::2], plots[1::2] = plots[:10], plots[10:]\n",
    "\n",
    "    Plotter.finish(\n",
    "        plots, figwidth=0.5,\n",
    "        grid_ncols=2, consistent_size=True,\n",
    "        save=True,\n",
    "    )\n",
    "\n",
    "plot(y_true_all, y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotter.configure(save_always=False)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_uq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
