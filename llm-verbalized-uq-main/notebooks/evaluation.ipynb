{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import widgets\n",
    "\n",
    "from benchmark import (\n",
    "    INVALID_ANSWER,\n",
    "    NO_ANSWER,\n",
    "    VALID_ANSWER,\n",
    "    aggregate_responses,\n",
    "    calibration_curve,\n",
    "    create_fig_accuracy_distribution,\n",
    "    create_fig_calibration_curve,\n",
    "    create_fig_calibration_ece,\n",
    "    create_fig_confidence_distribution,\n",
    "    create_fig_informativeness_diversity,\n",
    "    create_fig_meaningfulness_kldiv,\n",
    "    create_subplots,\n",
    "    detect_names_from_dict,\n",
    "    empirical_distr,\n",
    "    extract_predictions,\n",
    "    kl_div,\n",
    "    load_predictions,\n",
    "    load_responses,\n",
    "    load_responses_all,\n",
    "    plot_annotation,\n",
    "    plot_calibration_curve,\n",
    "    plot_confidence_histogram,\n",
    "    plot_heatmap,\n",
    "    save_fig,\n",
    "    save_predictions,\n",
    ")\n",
    "from utils_ext.plot import get_figlayout\n",
    "from utils_ext.tools import setup_logging\n",
    "from utils_ext.widgets import (\n",
    "    FileExplorerWidget,\n",
    "    build_widget_outputs,\n",
    "    display_table,\n",
    ")\n",
    "\n",
    "plt.ioff()\n",
    "setup_logging()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "PATH_OUTPUT = \"../results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CACHE = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: load responses and extract predictions\n",
    "responses_all = load_responses_all(f\"{PATH_OUTPUT}/responses\", dataset_cache=DATASET_CACHE)\n",
    "y_true_all, y_pred_all = extract_predictions(responses_all, sample=1000)\n",
    "\n",
    "# save_predictions(responses_all, f\"{PATH_OUTPUT}/predictions\", sample=None)\n",
    "# save_predictions(responses_all, f\"{PATH_OUTPUT}/predictions_sampled\", sample=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2: load predictions only\n",
    "y_true_all, y_pred_all = load_predictions(f\"{PATH_OUTPUT}/predictions_sampled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_responses_all_answer_stats(responses_all, save=None):\n",
    "    dataset_names, model_names, method_names = detect_names_from_dict(responses_all)\n",
    "\n",
    "    # compute\n",
    "    answer_stats = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: (0, 0, 0))))\n",
    "    answer_stats_over_dataset = defaultdict(lambda: defaultdict(lambda: (0, 0, 0)))\n",
    "    for model_name in model_names:\n",
    "        for method_name in method_names:\n",
    "            for dataset_name in dataset_names:\n",
    "                responses = responses_all[dataset_name][model_name][method_name]\n",
    "                if responses is None:\n",
    "                    continue\n",
    "                answer_stats[model_name][method_name][dataset_name] = (\n",
    "                    len(responses[VALID_ANSWER]),\n",
    "                    len(responses[NO_ANSWER]),\n",
    "                    len(responses[INVALID_ANSWER]),\n",
    "                )\n",
    "            answer_stats_over_dataset[model_name][method_name] = (\n",
    "                sum(answer_stats[model_name][method_name][dataset_name][0] for dataset_name in dataset_names),\n",
    "                sum(answer_stats[model_name][method_name][dataset_name][1] for dataset_name in dataset_names),\n",
    "                sum(answer_stats[model_name][method_name][dataset_name][2] for dataset_name in dataset_names),\n",
    "            )\n",
    "\n",
    "    # plot\n",
    "    fig1, ax1 = create_subplots(method_names, model_names, sharey=True)\n",
    "    for i, method_name in enumerate(method_names):\n",
    "        for j, model_name in enumerate(model_names):\n",
    "            n_valid_answer, n_no_answer, n_invalid_answer = answer_stats_over_dataset[model_name][method_name]\n",
    "            n_total = n_valid_answer + n_no_answer + n_invalid_answer\n",
    "\n",
    "            df = pd.DataFrame.from_dict(answer_stats[model_name][method_name], columns=[VALID_ANSWER, NO_ANSWER, INVALID_ANSWER], orient=\"index\")\n",
    "            df.plot.bar(ax=ax1[i, j], color={VALID_ANSWER: \"green\", NO_ANSWER: \"orange\", INVALID_ANSWER: \"red\"})\n",
    "            ax1[i, j].legend(loc=\"upper right\")\n",
    "\n",
    "            text = f\"valid:     {n_valid_answer}/{n_total}\"\n",
    "            text += f\"\\nno answer: {n_no_answer}/{n_total}\"\n",
    "            text += f\"\\ninvalid:   {n_invalid_answer}/{n_total}\"\n",
    "            plot_annotation(ax1[i, j], text)\n",
    "    if save:\n",
    "        save_fig(fig1, f\"{save}/answer_statistics.png\")\n",
    "    else:\n",
    "        plt.show(fig1)\n",
    "\n",
    "    percentage_valid_answer = np.zeros((len(method_names), len(model_names)))\n",
    "    percentage_no_answer = np.zeros((len(method_names), len(model_names)))\n",
    "    for i, method_name in enumerate(method_names):\n",
    "        for j, model_name in enumerate(model_names):\n",
    "            n_valid_answer, n_no_answer, n_invalid_answer = answer_stats_over_dataset[model_name][method_name]\n",
    "            n_total = n_valid_answer + n_no_answer + n_invalid_answer\n",
    "            percentage_valid_answer[i, j] = n_valid_answer / n_total if n_total > 0 else None\n",
    "            percentage_no_answer[i, j] = n_no_answer / n_total if n_total > 0 else None\n",
    "    fig2, ax2 = plt.subplots(**get_figlayout(ncols=2, width=7, ratio=(len(method_names), len(model_names))), layout=\"constrained\")\n",
    "    ax2[0].set_title(\"% valid answer\")\n",
    "    plot_heatmap(ax2[0], percentage_valid_answer, method_names, model_names, vmin=0, vmax=1, cmap=\"RdYlGn\")\n",
    "    ax2[1].set_title(\"% no answer\")\n",
    "    plot_heatmap(ax2[1], percentage_no_answer, method_names, model_names, vmin=0, vmax=1, cmap=\"RdYlGn_r\")\n",
    "    if save:\n",
    "        save_fig(fig2, f\"{save}/answer_statistics_heatmap.png\")\n",
    "    else:\n",
    "        plt.show(fig2)\n",
    "\n",
    "display_responses_all_answer_stats(responses_all, save=f\"{PATH_OUTPUT}/plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_responses_all_over_datasets(y_true_all, y_pred_all, n_bins=20, save=None):\n",
    "    dataset_names, model_names, method_names = detect_names_from_dict(y_true_all)\n",
    "\n",
    "    # compute\n",
    "    scores = {\n",
    "        \"ece\": np.zeros((len(method_names), len(model_names))),\n",
    "        \"accuracy\": np.zeros((len(method_names), len(model_names))),\n",
    "        \"confidence\": np.zeros((len(method_names), len(model_names))),\n",
    "        \"confidence_n_distinct\": np.zeros((len(method_names), len(model_names))),\n",
    "        \"confidence_variance\": np.zeros((len(method_names), len(model_names))),\n",
    "        \"kl_div_over_dataset\": np.zeros((len(method_names), len(model_names))),\n",
    "    }\n",
    "    for i, method_name in enumerate(method_names):\n",
    "        for j, model_name in enumerate(model_names):\n",
    "            y_true = aggregate_responses(y_true_all, dataset_names, model_name, method_name)\n",
    "            y_pred = aggregate_responses(y_pred_all, dataset_names, model_name, method_name)\n",
    "\n",
    "            prob_true, prob_pred, bins, bin_count = calibration_curve(y_true, y_pred, n_bins=n_bins)\n",
    "            ece = np.sum(bin_count / len(y_true) * np.abs(prob_true - prob_pred), where=bin_count > 0)\n",
    "            scores[\"ece\"][i, j] = ece\n",
    "            scores[\"accuracy\"][i, j] = np.mean(y_true)\n",
    "            scores[\"confidence\"][i, j] = np.mean(y_pred)\n",
    "            scores[\"confidence_n_distinct\"][i, j] = len(np.unique(y_pred))\n",
    "            scores[\"confidence_variance\"][i, j] = np.std(y_pred)\n",
    "            scores[\"kl_div_over_dataset\"][i, j] = np.mean([\n",
    "                kl_div(\n",
    "                    empirical_distr(y_pred_all[dataset_name][model_name][method_name], n_bins),\n",
    "                    empirical_distr(aggregate_responses(y_pred_all, dataset_names, model_name, method_name), n_bins)\n",
    "                )\n",
    "                for dataset_name in dataset_names\n",
    "            ])\n",
    "\n",
    "    # plot\n",
    "    create_fig_accuracy_distribution(\"all_over_datasets\", y_true_all, [dataset_names, model_names, method_names], 2, 1, n_bins, save=save)\n",
    "    create_fig_confidence_distribution(\"all_over_datasets\", y_pred_all, [dataset_names, model_names, method_names], 2, 1, n_bins, save=save)\n",
    "    create_fig_calibration_curve(\"all_over_datasets\", y_true_all, y_pred_all, [dataset_names, model_names, method_names], 2, 1, n_bins, save=save)\n",
    "\n",
    "    create_fig_calibration_ece(\"all_over_datasets\", scores, method_names, model_names, save=save)\n",
    "    create_fig_informativeness_diversity(\"all_over_datasets\", scores, method_names, model_names, save=save)\n",
    "    create_fig_meaningfulness_kldiv(\"all_over_datasets\", scores, method_names, model_names, save=save)\n",
    "\n",
    "display_responses_all_over_datasets(y_true_all, y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_responses_all_over_models(y_true_all, y_pred_all, n_bins=20, save=None):\n",
    "    dataset_names, model_names, method_names = detect_names_from_dict(y_true_all)\n",
    "\n",
    "    # compute\n",
    "    scores = {\n",
    "        \"ece\": np.zeros((len(method_names), len(dataset_names))),\n",
    "        \"accuracy\": np.zeros((len(method_names), len(dataset_names))),\n",
    "        \"confidence\": np.zeros((len(method_names), len(dataset_names))),\n",
    "        \"confidence_n_distinct\": np.zeros((len(method_names), len(dataset_names))),\n",
    "        \"confidence_variance\": np.zeros((len(method_names), len(dataset_names))),\n",
    "        \"kl_div_over_dataset\": np.zeros((len(method_names), len(dataset_names))),\n",
    "    }\n",
    "    for i, method_name in enumerate(method_names):\n",
    "        for j, dataset_name in enumerate(dataset_names):\n",
    "            y_true = aggregate_responses(y_true_all, dataset_name, model_names, method_name)\n",
    "            y_pred = aggregate_responses(y_pred_all, dataset_name, model_names, method_name)\n",
    "\n",
    "            prob_true, prob_pred, bins, bin_count = calibration_curve(y_true, y_pred, n_bins=n_bins)\n",
    "            ece = np.sum(bin_count / len(y_true) * np.abs(prob_true - prob_pred), where=bin_count > 0)\n",
    "            scores[\"ece\"][i, j] = ece\n",
    "            scores[\"accuracy\"][i, j] = np.mean(y_true)\n",
    "            scores[\"confidence\"][i, j] = np.mean(y_pred)\n",
    "            scores[\"confidence_n_distinct\"][i, j] = len(np.unique(y_pred))\n",
    "            scores[\"confidence_variance\"][i, j] = np.std(y_pred)\n",
    "            scores[\"kl_div_over_dataset\"][i, j] = np.mean([\n",
    "                kl_div(\n",
    "                    empirical_distr(y_pred_all[dataset_name][model_name][method_name], n_bins),\n",
    "                    empirical_distr(aggregate_responses(y_pred_all, dataset_names, model_name, method_name), n_bins)\n",
    "                )\n",
    "                for model_name in model_names\n",
    "            ])\n",
    "\n",
    "    # plot\n",
    "    create_fig_accuracy_distribution(\"all_over_models\", y_true_all, [dataset_names, model_names, method_names], 2, 0, n_bins, save=save)\n",
    "    create_fig_confidence_distribution(\"all_over_models\", y_pred_all, [dataset_names, model_names, method_names], 2, 0, n_bins, save=save)\n",
    "    create_fig_calibration_curve(\"all_over_models\", y_true_all, y_pred_all, [dataset_names, model_names, method_names], 2, 0, n_bins, save=save)\n",
    "\n",
    "    create_fig_calibration_ece(\"all_over_models\", scores, method_names, dataset_names, save=save)\n",
    "    create_fig_informativeness_diversity(\"all_over_models\", scores, method_names, dataset_names, save=save)\n",
    "    create_fig_meaningfulness_kldiv(\"all_over_models\", scores, method_names, dataset_names, save=save)\n",
    "\n",
    "display_responses_all_over_models(y_true_all, y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_responses_all_over_methods(y_true_all, y_pred_all, n_bins=20, save=None):\n",
    "    dataset_names, model_names, method_names = detect_names_from_dict(y_true_all)\n",
    "\n",
    "    # compute\n",
    "    scores = {\n",
    "        \"ece\": np.zeros((len(dataset_names), len(model_names))),\n",
    "        \"accuracy\": np.zeros((len(dataset_names), len(model_names))),\n",
    "        \"confidence\": np.zeros((len(dataset_names), len(model_names))),\n",
    "        \"confidence_n_distinct\": np.zeros((len(dataset_names), len(model_names))),\n",
    "        \"confidence_variance\": np.zeros((len(dataset_names), len(model_names))),\n",
    "        \"kl_div_over_dataset\": np.zeros((len(dataset_names), len(model_names))),\n",
    "    }\n",
    "    for i, dataset_name in enumerate(dataset_names):\n",
    "        for j, model_name in enumerate(model_names):\n",
    "            y_true = aggregate_responses(y_true_all, dataset_name, model_name, method_names)\n",
    "            y_pred = aggregate_responses(y_pred_all, dataset_name, model_name, method_names)\n",
    "\n",
    "            prob_true, prob_pred, bins, bin_count = calibration_curve(y_true, y_pred, n_bins=n_bins)\n",
    "            ece = np.sum(bin_count / len(y_true) * np.abs(prob_true - prob_pred), where=bin_count > 0)\n",
    "            scores[\"ece\"][i, j] = ece\n",
    "            scores[\"accuracy\"][i, j] = np.mean(y_true)\n",
    "            scores[\"confidence\"][i, j] = np.mean(y_pred)\n",
    "            scores[\"confidence_n_distinct\"][i, j] = len(np.unique(y_pred))\n",
    "            scores[\"confidence_variance\"][i, j] = np.std(y_pred)\n",
    "            scores[\"kl_div_over_dataset\"][i, j] = np.mean([\n",
    "                kl_div(\n",
    "                    empirical_distr(y_pred_all[dataset_name][model_name][method_name], n_bins),\n",
    "                    empirical_distr(aggregate_responses(y_pred_all, dataset_names, model_name, method_name), n_bins)\n",
    "                )\n",
    "                for method_name in method_names\n",
    "            ])\n",
    "\n",
    "    # plot\n",
    "    create_fig_accuracy_distribution(\"all_over_methods\", y_true_all, [dataset_names, model_names, method_names], 0, 1, n_bins, save=save)\n",
    "    create_fig_confidence_distribution(\"all_over_methods\", y_pred_all, [dataset_names, model_names, method_names], 0, 1, n_bins, save=save)\n",
    "    create_fig_calibration_curve(\"all_over_methods\", y_true_all, y_pred_all, [dataset_names, model_names, method_names], 0, 1, n_bins, save=save)\n",
    "\n",
    "    create_fig_calibration_ece(\"all_over_methods\", scores, dataset_names, model_names, save=save)\n",
    "    create_fig_informativeness_diversity(\"all_over_methods\", scores, dataset_names, model_names, save=save)\n",
    "    create_fig_meaningfulness_kldiv(\"all_over_methods\", scores, dataset_names, model_names, save=save)\n",
    "\n",
    "display_responses_all_over_methods(y_true_all, y_pred_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_responses_per_model(y_true_all, y_pred_all, model_name, n_bins=20, save=None):\n",
    "    dataset_names, _, method_names = detect_names_from_dict(y_true_all)\n",
    "\n",
    "    # compute\n",
    "    scores = {\n",
    "        \"ece\": np.zeros((len(method_names), len(dataset_names))),\n",
    "        \"accuracy\": np.zeros((len(method_names), len(dataset_names))),\n",
    "        \"confidence\": np.zeros((len(method_names), len(dataset_names))),\n",
    "        \"confidence_n_distinct\": np.zeros((len(method_names), len(dataset_names))),\n",
    "        \"confidence_variance\": np.zeros((len(method_names), len(dataset_names))),\n",
    "        \"kl_div_over_dataset\": np.zeros((len(method_names), len(dataset_names))),\n",
    "    }\n",
    "    for i, method_name in enumerate(method_names):\n",
    "        for j, dataset_name in enumerate(dataset_names):\n",
    "            y_true = y_true_all[dataset_name][model_name][method_name]\n",
    "            y_pred = y_pred_all[dataset_name][model_name][method_name]\n",
    "            prob_true, prob_pred, bins, bin_count = calibration_curve(y_true, y_pred, n_bins=n_bins)\n",
    "            ece = np.sum(bin_count / len(y_true) * np.abs(prob_true - prob_pred), where=bin_count > 0)\n",
    "            scores[\"accuracy\"][i, j] = np.mean(y_true)\n",
    "            scores[\"confidence\"][i, j] = np.mean(y_pred)\n",
    "            scores[\"ece\"][i, j] = ece\n",
    "            scores[\"confidence_n_distinct\"][i, j] = len(np.unique(y_pred))\n",
    "            scores[\"confidence_variance\"][i, j] = np.std(y_pred)\n",
    "            scores[\"kl_div_over_dataset\"][i, j] = kl_div(\n",
    "                empirical_distr(y_pred_all[dataset_name][model_name][method_name], n_bins),\n",
    "                empirical_distr(aggregate_responses(y_pred_all, dataset_names, model_name, method_name), n_bins),\n",
    "            )\n",
    "\n",
    "    # plot\n",
    "    create_fig_accuracy_distribution(f\"per_model/{model_name}\", y_true_all, [dataset_names, model_name, method_names], 2, 0, n_bins, save=save)\n",
    "    create_fig_confidence_distribution(f\"per_model/{model_name}\", y_pred_all, [dataset_names, model_name, method_names], 2, 0, n_bins, save=save)\n",
    "    create_fig_calibration_curve(f\"per_model/{model_name}\", y_true_all, y_pred_all, [dataset_names, model_name, method_names], 2, 0, n_bins, save=save)\n",
    "\n",
    "    create_fig_calibration_ece(f\"per_model/{model_name}\", scores, method_names, dataset_names, save=save)\n",
    "    create_fig_informativeness_diversity(f\"per_model/{model_name}\", scores, method_names, dataset_names, save=save)\n",
    "    create_fig_meaningfulness_kldiv(f\"per_model/{model_name}\", scores, method_names, dataset_names, save=save)\n",
    "\n",
    "display_responses_per_model(y_true_all, y_pred_all, \"gemma1.1-2b-it\", n_bins=10)\n",
    "display_responses_per_model(y_true_all, y_pred_all, \"gemma1.1-7b-it\", n_bins=10)\n",
    "display_responses_per_model(y_true_all, y_pred_all, \"llama3-8b-instruct\", n_bins=10)\n",
    "display_responses_per_model(y_true_all, y_pred_all, \"llama3-70b-instruct\", n_bins=10)\n",
    "display_responses_per_model(y_true_all, y_pred_all, \"qwen1.5-7b-chat\", n_bins=10)\n",
    "display_responses_per_model(y_true_all, y_pred_all, \"qwen1.5-32b-chat\", n_bins=10)\n",
    "display_responses_per_model(y_true_all, y_pred_all, \"qwen1.5-72b-chat\", n_bins=10)\n",
    "display_responses_per_model(y_true_all, y_pred_all, \"qwen1.5-110b-chat\", n_bins=10)\n",
    "display_responses_per_model(y_true_all, y_pred_all, \"gpt3.5-turbo\", n_bins=10)\n",
    "display_responses_per_model(y_true_all, y_pred_all, \"gpt4o-mini\", n_bins=10)\n",
    "display_responses_per_model(y_true_all, y_pred_all, \"gpt4o\", n_bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-method analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_responses_per_method(y_true_all, y_pred_all, method_name, n_bins=20, save=None):\n",
    "    dataset_names, model_names, _ = detect_names_from_dict(y_true_all)\n",
    "\n",
    "    # compute\n",
    "    scores = {\n",
    "        \"ece\": np.zeros((len(dataset_names), len(model_names))),\n",
    "        \"accuracy\": np.zeros((len(dataset_names), len(model_names))),\n",
    "        \"confidence\": np.zeros((len(dataset_names), len(model_names))),\n",
    "        \"confidence_n_distinct\": np.zeros((len(dataset_names), len(model_names))),\n",
    "        \"confidence_variance\": np.zeros((len(dataset_names), len(model_names))),\n",
    "        \"kl_div_over_dataset\": np.zeros((len(dataset_names), len(model_names))),\n",
    "    }\n",
    "    for i, dataset_name in enumerate(dataset_names):\n",
    "        for j, model_name in enumerate(model_names):\n",
    "            y_true = y_true_all[dataset_name][model_name][method_name]\n",
    "            y_pred = y_pred_all[dataset_name][model_name][method_name]\n",
    "            prob_true, prob_pred, bins, bin_count = calibration_curve(y_true, y_pred, n_bins=n_bins)\n",
    "            ece = np.sum(bin_count / len(y_true) * np.abs(prob_true - prob_pred), where=bin_count > 0)\n",
    "            scores[\"accuracy\"][i, j] = np.mean(y_true)\n",
    "            scores[\"confidence\"][i, j] = np.mean(y_pred)\n",
    "            scores[\"ece\"][i, j] = ece\n",
    "            scores[\"confidence_n_distinct\"][i, j] = len(np.unique(y_pred))\n",
    "            scores[\"confidence_variance\"][i, j] = np.std(y_pred)\n",
    "            scores[\"kl_div_over_dataset\"][i, j] = kl_div(\n",
    "                empirical_distr(y_pred_all[dataset_name][model_name][method_name], n_bins),\n",
    "                empirical_distr(aggregate_responses(y_pred_all, dataset_names, model_name, method_name), n_bins),\n",
    "            )\n",
    "\n",
    "    # plot\n",
    "    create_fig_accuracy_distribution(f\"per_method/{method_name}\", y_true_all, [dataset_names, model_names, method_name], 0, 1, n_bins, save=save)\n",
    "    create_fig_confidence_distribution(f\"per_method/{method_name}\", y_pred_all, [dataset_names, model_names, method_name], 0, 1, n_bins, save=save)\n",
    "    create_fig_calibration_curve(f\"per_method/{method_name}\", y_true_all, y_pred_all, [dataset_names, model_names, method_name], 0, 1, n_bins, save=save)\n",
    "\n",
    "    create_fig_calibration_ece(f\"per_method/{method_name}\", scores, dataset_names, model_names, save=save)\n",
    "    create_fig_informativeness_diversity(f\"per_method/{method_name}\", scores, dataset_names, model_names, save=save)\n",
    "    create_fig_meaningfulness_kldiv(f\"per_method/{method_name}\", scores, dataset_names, model_names, save=save)\n",
    "\n",
    "display_responses_per_method(y_true_all, y_pred_all, \"basic_1s\", n_bins=10, save=f\"{PATH_OUTPUT}/plots\")\n",
    "display_responses_per_method(y_true_all, y_pred_all, \"basic_1s_probscore\", n_bins=10, save=f\"{PATH_OUTPUT}/plots\")\n",
    "display_responses_per_method(y_true_all, y_pred_all, \"basic_1s_1shot\", n_bins=10, save=f\"{PATH_OUTPUT}/plots\")\n",
    "display_responses_per_method(y_true_all, y_pred_all, \"basic_1s_5shot\", n_bins=10, save=f\"{PATH_OUTPUT}/plots\")\n",
    "display_responses_per_method(y_true_all, y_pred_all, \"advanced_1s\", n_bins=10, save=f\"{PATH_OUTPUT}/plots\")\n",
    "display_responses_per_method(y_true_all, y_pred_all, \"advanced_1s_probscore\", n_bins=10, save=f\"{PATH_OUTPUT}/plots\")\n",
    "display_responses_per_method(y_true_all, y_pred_all, \"combo_1s_v2\", n_bins=10, save=f\"{PATH_OUTPUT}/plots\")\n",
    "display_responses_per_method(y_true_all, y_pred_all, \"tian2023just_1s_top1\", n_bins=10, save=f\"{PATH_OUTPUT}/plots\")\n",
    "display_responses_per_method(y_true_all, y_pred_all, \"tian2023just_1s_top4\", n_bins=10, save=f\"{PATH_OUTPUT}/plots\")\n",
    "display_responses_per_method(y_true_all, y_pred_all, \"xiong2023can_vanilla\", n_bins=10, save=f\"{PATH_OUTPUT}/plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responses viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_responses_viewer():\n",
    "    def display_responses(*args):\n",
    "        if fe_path.value is None:\n",
    "            return\n",
    "\n",
    "        path_components = fe_path.value.split(os.sep)\n",
    "        path_responses = os.sep.join(path_components[:-3])\n",
    "        dataset_name = path_components[-3]\n",
    "        model_name = path_components[-2]\n",
    "        method_name = os.path.splitext(path_components[-1])[0]\n",
    "\n",
    "        # load dataset and responses\n",
    "        with out_log:\n",
    "            out_log.clear_output(wait=True)\n",
    "            responses = load_responses(path_responses, dataset_name, model_name, method_name, dataset_cache=DATASET_CACHE)\n",
    "            responses_valid_correct = [(prompt, response) for prompt, response in responses[VALID_ANSWER] if response[\"is_correct\"] == 1]\n",
    "            responses_valid_incorrect = [(prompt, response) for prompt, response in responses[VALID_ANSWER] if response[\"is_correct\"] < 1]\n",
    "        # display plot\n",
    "        with out_responses:\n",
    "            out_responses.clear_output(wait=True)\n",
    "\n",
    "            y_true = [response[\"is_correct\"] for _, response in responses[VALID_ANSWER]]\n",
    "            y_pred = [response[\"confidence\"] for _, response in responses[VALID_ANSWER]]\n",
    "\n",
    "            fig, ax = plt.subplots(**get_figlayout(ncols=2, width=4), layout=\"constrained\")\n",
    "            fig.suptitle(f\"{dataset_name} / {model_name} / {method_name}\")\n",
    "            ax[0].set(\n",
    "                title=\"Histogram of confidence scores\",\n",
    "                xlabel=\"confidence\",\n",
    "                ylabel=\"count\",\n",
    "            )\n",
    "            ax[0].title.set_fontsize(\"medium\")\n",
    "            plot_confidence_histogram(ax[0], y_pred, n_bins=20)\n",
    "            ax[1].set(\n",
    "                title=\"Calibration curve of confidence scores\",\n",
    "                xlabel=\"confidence\",\n",
    "                ylabel=\"accuracy\",\n",
    "            )\n",
    "            ax[1].title.set_fontsize(\"medium\")\n",
    "            plot_calibration_curve(ax[1], y_true, y_pred, n_bins=20)\n",
    "            plt.show(fig)\n",
    "\n",
    "            n_valid_answer = len(responses[VALID_ANSWER])\n",
    "            n_valid_answer_correct = len(responses_valid_correct)\n",
    "            n_valid_answer_incorrect = len(responses_valid_incorrect)\n",
    "            n_no_answer = len(responses[NO_ANSWER])\n",
    "            n_invalid_answer = len(responses[INVALID_ANSWER])\n",
    "            n_total = n_valid_answer + n_no_answer + n_invalid_answer\n",
    "            print(f\"valid answers:   {n_valid_answer}/{n_total}\")\n",
    "            print(f\"  correct:         {n_valid_answer_correct}/{n_valid_answer}\")\n",
    "            print(f\"  incorrect:       {n_valid_answer_incorrect}/{n_valid_answer}\")\n",
    "            print(f\"no answers:      {n_no_answer}/{n_total}\")\n",
    "            print(f\"invalid answers: {n_invalid_answer}/{n_total}\")\n",
    "        # display responses\n",
    "        with out_responses_tables[\"valid_correct\"]:\n",
    "            out_responses_tables[\"valid_correct\"].clear_output(wait=True)\n",
    "            display_responses_table(responses_valid_correct)\n",
    "        with out_responses_tables[\"valid_incorrect\"]:\n",
    "            out_responses_tables[\"valid_incorrect\"].clear_output(wait=True)\n",
    "            display_responses_table(responses_valid_incorrect)\n",
    "        with out_responses_tables[\"no_answer\"]:\n",
    "            out_responses_tables[\"no_answer\"].clear_output(wait=True)\n",
    "            display_responses_table(responses[NO_ANSWER])\n",
    "        with out_responses_tables[\"invalid\"]:\n",
    "            out_responses_tables[\"invalid\"].clear_output(wait=True)\n",
    "            display_responses_table(responses[INVALID_ANSWER])\n",
    "\n",
    "    def display_responses_table(responses):\n",
    "        responses = [\n",
    "            {\n",
    "                \"id\": prompt[\"id\"],\n",
    "                \"prompt\": prompt[\"content\"],\n",
    "                \"correct answer\": prompt[\"correct_answer\"],\n",
    "                \"response\": \"\\n----------\\n\".join(response[\"responses\"]),\n",
    "                \"answer\": response[\"answer\"],\n",
    "                \"confidence\": response[\"confidence\"],\n",
    "                \"is_correct\": response[\"is_correct\"],\n",
    "            } for prompt, response in responses\n",
    "        ]\n",
    "        responses = pd.DataFrame(responses).replace(\"\\n\", \"<br>\", regex=True)\n",
    "        display_table(responses, html_align=\"left\")\n",
    "\n",
    "    fe_path = FileExplorerWidget(\n",
    "        [\"path\", \"dataset\", \"model\", \"method\"],\n",
    "        default=f\"{PATH_OUTPUT}/responses\",\n",
    "    )\n",
    "    fe_path.observe(display_responses)\n",
    "    out_log = widgets.Output()\n",
    "    out_responses = widgets.Output()\n",
    "    out_responses_tables = build_widget_outputs([\"valid_correct\", \"valid_incorrect\", \"no_answer\", \"invalid\"], layout={\"max_height\": \"500px\", \"overflow\": \"auto\"})\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        fe_path,\n",
    "        out_log,\n",
    "        out_responses,\n",
    "        widgets.HTML(\"<b>valid answers (correct)</b>\"),\n",
    "        out_responses_tables[\"valid_correct\"],\n",
    "        widgets.HTML(\"<b>valid answers (incorrect)</b>\"),\n",
    "        out_responses_tables[\"valid_incorrect\"],\n",
    "        widgets.HTML(\"<b>no answers</b>\"),\n",
    "        out_responses_tables[\"no_answer\"],\n",
    "        widgets.HTML(\"<b>invalid answers</b>\"),\n",
    "        out_responses_tables[\"invalid\"],\n",
    "    ]))\n",
    "\n",
    "run_responses_viewer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_uq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
