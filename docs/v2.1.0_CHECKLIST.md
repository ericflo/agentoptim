# AgentOptim v2.1.0 Release Checklist

This document outlines the tasks required to complete the AgentOptim v2.1.0 release, scheduled for July 2025.

## Timeline

| Milestone | Date | Description |
|-----------|------|-------------|
| Planning | March 2025 | Finalize v2.1.0 feature set and test improvement plan |
| Alpha | April 2025 | Begin implementation of compatibility layer removal |
| Test Sprint | May 2025 | Focus on test improvements and validation | 
| Beta | June 2025 | Feature complete with all tests passing |
| Release | July 2025 | Official v2.1.0 release |

## 1. Remove Compatibility Layer

- [ ] **Remove `compat.py` module**
  - [ ] Delete the file
  - [ ] Remove any imports of compat.py throughout the codebase
  - [ ] Verify no remaining references to compatibility functions

- [ ] **Remove compatibility layer tests**
  - [ ] Delete `test_compat.py`
  - [ ] Remove any compatibility layer tests in other test files
  - [ ] Verify all tests pass after removal

- [ ] **Remove deprecated examples**
  - [ ] Delete `examples/deprecated_examples` directory
  - [ ] Update examples README.md to remove references to deprecated examples

- [ ] **Update documentation**
  - [ ] Remove any remaining mentions of compatibility layer
  - [ ] Update MIGRATION_GUIDE.md to note compat layer has been removed
  - [ ] Update version references in all documentation

## 2. Improve Test Coverage

### Server Module (Current: 66% → Target: 85%+)

- [ ] **Add tests for `manage_evalset_tool`**
  - [ ] Test input validation
  - [ ] Test error handling
  - [ ] Test each action (create, get, list, update, delete)
  - [ ] Test with various parameter combinations

- [ ] **Add tests for `run_evalset_tool`**
  - [ ] Test with different conversation formats
  - [ ] Test with different model parameters
  - [ ] Test error handling
  - [ ] Test parallel execution configuration

### Runner Module (Current: 74% → Target: 85%+)

- [ ] **Add tests for model interaction**
  - [ ] Test with different model providers
  - [ ] Test timeout handling
  - [ ] Test rate limit handling
  - [ ] Test error handling for model API failures

- [ ] **Add tests for parallel processing**
  - [ ] Test with various max_parallel settings
  - [ ] Test handling of failures in parallel execution
  - [ ] Test performance with large question sets

### Integration Tests

- [ ] **Add end-to-end workflow tests**
  - [ ] Test creating an evalset and then running it
  - [ ] Test updating an evalset and verifying changes affect evaluation
  - [ ] Test multiple concurrent evaluations

- [ ] **Add error recovery tests**
  - [ ] Test handling of temporary failures
  - [ ] Test resuming evaluations after interruptions

## 3. Enhance Documentation

- [ ] **Create API reference**
  - [ ] Document `manage_evalset_tool` parameters and return values
  - [ ] Document `run_evalset_tool` parameters and return values
  - [ ] Document template format and variables
  - [ ] Document result format and structure

- [ ] **Add advanced examples**
  - [ ] Create example for custom template formats
  - [ ] Create example for comparing multiple LLM responses
  - [ ] Create example for integration with popular LLM frameworks

- [ ] **Improve tutorial**
  - [ ] Add more use cases
  - [ ] Create step-by-step workflow tutorial
  - [ ] Add troubleshooting section

## 4. Performance Optimizations

- [ ] **Improve caching**
  - [ ] Implement LRU cache for frequently accessed EvalSets
  - [ ] Add cache invalidation for modified EvalSets
  - [ ] Add cache size configuration options

- [ ] **Optimize parallel execution**
  - [ ] Improve job scheduling algorithm
  - [ ] Add adaptive parallelism based on available resources
  - [ ] Implement batching for similar evaluation questions

- [ ] **Reduce memory usage**
  - [ ] Optimize storage format for large EvalSets
  - [ ] Implement streaming for large result sets
  - [ ] Reduce duplication in data structures

## 5. Release Process

- [ ] **Update version number**
  - [ ] Update version in `__init__.py`
  - [ ] Update version in setup.py
  - [ ] Update version in documentation

- [ ] **Run final tests**
  - [ ] Run full test suite
  - [ ] Verify code coverage meets targets
  - [ ] Run performance benchmarks

- [ ] **Prepare release notes**
  - [ ] Document all changes since v2.0
  - [ ] Highlight breaking changes (compatibility layer removal)
  - [ ] Include migration guidance

- [ ] **Package for distribution**
  - [ ] Build package
  - [ ] Test installation in clean environment
  - [ ] Publish to PyPI

## Success Criteria

- [ ] Compatibility layer completely removed
- [ ] Test coverage at least 95% overall
- [ ] Server module coverage at least 85%
- [ ] Runner module coverage at least 85%
- [ ] Performance improvements measurable and documented
- [ ] Documentation updated and comprehensive
- [ ] All tests passing

---

This checklist will be reviewed and updated during the planning phase in March 2025 to incorporate any additional requirements or changes in priorities.