# AgentOptim v2.1.0 Release Checklist

This document outlines the tasks required to complete the AgentOptim v2.1.0 release, scheduled for July 2025.

## Timeline

| Milestone | Date | Description |
|-----------|------|-------------|
| Planning | March 2025 | Finalize v2.1.0 feature set and test improvement plan |
| Alpha | April 2025 | Begin implementation of compatibility layer removal |
| Test Sprint | May 2025 | Focus on test improvements and validation | 
| Beta | June 2025 | Feature complete with all tests passing |
| Release | July 2025 | Official v2.1.0 release |

## 1. Remove Compatibility Layer

- [x] **Remove `compat.py` module**
  - [x] Delete the file
  - [x] Remove any imports of compat.py throughout the codebase
  - [x] Verify no remaining references to compatibility functions

- [x] **Remove compatibility layer tests**
  - [x] Delete `test_compat.py`
  - [x] Remove any compatibility layer tests in other test files
  - [x] Verify all tests pass after removal

- [x] **Remove deprecated examples**
  - [x] Delete `examples/deprecated_examples` directory
  - [x] Update examples README.md to remove references to deprecated examples

- [x] **Update documentation**
  - [x] Remove any remaining mentions of compatibility layer
  - [x] Update MIGRATION_GUIDE.md to note compat layer has been removed
  - [x] Update version references in all documentation

## 2. Improve Test Coverage

### Server Module (Current: 92% â†’ Target: 85%+ âœ…)

- [x] **Add tests for `manage_evalset_tool`**
  - [x] Test input validation
  - [x] Test error handling
  - [x] Test each action (create, get, list, update, delete)
  - [x] Test with various parameter combinations

- [x] **Add tests for `run_evalset_tool`**
  - [x] Test with different conversation formats
  - [x] Test with different model parameters
  - [x] Test error handling
  - [x] Test parallel execution configuration

### Runner Module (Current: 56% â†’ Target: 85%+) ðŸš§

- [x] **Add tests for model interaction**
  - [x] Test with different model providers
  - [ ] Test timeout handling
  - [x] Test rate limit handling
  - [x] Test error handling for model API failures

- [x] **Add tests for parallel processing**
  - [x] Test with various max_parallel settings
  - [x] Test handling of failures in parallel execution
  - [x] Test performance with large question sets

- [x] **Add tests for error handling**
  - [x] Test with invalid JSON responses
  - [x] Test with empty question lists
  - [x] Test with invalid max_parallel values

*Note: All previously skipped tests have been implemented and enabled, improving the coverage from 49% to 56%. There's still more work needed to reach the 85% target, particularly in testing the call_llm_api function which contains complex error handling logic.*

### Integration Tests

- [x] **Add end-to-end workflow tests**
  - [x] Test creating an evalset and then running it
  - [x] Test updating an evalset and verifying changes affect evaluation
  - [x] Test multiple concurrent evaluations

- [x] **Add error recovery tests**
  - [x] Test handling of temporary failures
  - [x] Test handling of rate limit errors
  - [x] Test handling of partial failures during evaluation

## 3. Enhance Documentation

- [ ] **Create API reference**
  - [ ] Document `manage_evalset_tool` parameters and return values
  - [ ] Document `run_evalset_tool` parameters and return values
  - [ ] Document template format and variables
  - [ ] Document result format and structure

- [ ] **Add advanced examples**
  - [ ] Create example for custom template formats
  - [ ] Create example for comparing multiple LLM responses
  - [ ] Create example for integration with popular LLM frameworks

- [ ] **Improve tutorial**
  - [ ] Add more use cases
  - [ ] Create step-by-step workflow tutorial
  - [ ] Add troubleshooting section

## 4. Performance Optimizations

- [ ] **Improve caching**
  - [ ] Implement LRU cache for frequently accessed EvalSets
  - [ ] Add cache invalidation for modified EvalSets
  - [ ] Add cache size configuration options

- [ ] **Optimize parallel execution**
  - [ ] Improve job scheduling algorithm
  - [ ] Add adaptive parallelism based on available resources
  - [ ] Implement batching for similar evaluation questions

- [ ] **Reduce memory usage**
  - [ ] Optimize storage format for large EvalSets
  - [ ] Implement streaming for large result sets
  - [ ] Reduce duplication in data structures

## 5. Release Process

- [ ] **Update version number**
  - [ ] Update version in `__init__.py`
  - [ ] Update version in setup.py
  - [ ] Update version in documentation

- [ ] **Run final tests**
  - [ ] Run full test suite
  - [ ] Verify code coverage meets targets
  - [ ] Run performance benchmarks

- [ ] **Prepare release notes**
  - [ ] Document all changes since v2.0
  - [ ] Highlight breaking changes (compatibility layer removal)
  - [ ] Include migration guidance

- [ ] **Package for distribution**
  - [ ] Build package
  - [ ] Test installation in clean environment
  - [ ] Publish to PyPI

## Success Criteria

- [x] Compatibility layer completely removed
- [x] Test coverage at least 95% overall (Current: 95%)
- [x] Server module coverage at least 85% (Current: 92%)
- [ðŸ”¶] Runner module coverage at least 85% (Current: 75%, significant improvement)
- [ ] Performance improvements measurable and documented
- [ ] Documentation updated and comprehensive
- [x] All tests passing (no skipped tests)

---

This checklist will be reviewed and updated during the planning phase in March 2025 to incorporate any additional requirements or changes in priorities.